<div class="case-study-insider">
  <h3 class="section-subheading">Insider.in</h3>
  <p class="section-content">
    PayTM Insider is India's largest online ticketing and events company. 
    Insider approached nilenso in early 2018 with a concept for an entirely new business model.
    Millions of fans were already coming to Insider's live events.
    Insider wanted to bring those events online.
    Could nilenso help them host online events --- with software hosting all those millions of fans <i>at once</i>?
  </p>

  <h3 class="section-miniheading">The Problem</h3>
  <p class="section-content">
    It wasn't a foregone conclusion that we could build such a system.
    We certainly had never built one like it before.
    Even software like Twitch.
    tv, dedicated to streaming video to simultaneous users, lives in the realm of "millions of simultaneous users".
    Insider's live events, such as NH7 Weekender, were known to draw crowds large enough to bring commercial telecommunications infrastructure to its knees.
    Building a platform to host even larger online events? We were intrigued.
  </p>
  <br/>
  <p class="section-content">
    The project began with a disarmingly simple problem statement: "Stream interactive video to one million concurrent users." 
    Outside of a real emergency, we would never normally work weekends, but we had worked with the team from Insider before and we were just as excited as they were about solving this puzzle.
    We spent that first weekend split into teams, battling a GoLang prototype against an Elixir prototype.
    (By which, we really mean Erlang... by which, we really mean BEAM/ERTS/OTP, Erlang's Virtual Machine (VM)/Runtime/Core Libraries. More on that later.) 
    Our usual weapon of choice, Clojure, wasn't a good fit for a system that demanded one million simultaneous connections because the JVM's scaling models were simply too heavy.
    GoLang has some wonderful concurrency primitives, but they proved no match for Erlang's ultralightweight processes, OTP (which, for purely historical reasons, stands for "Open Telecom Platform") tooling, and in-your-face distributed systems approach.
    By the end of the weekend, Elixir had won the brawl and the project had a name: <i>ngage</i>.
  </p>

  <h3 class="section-miniheading">The Solution</h3>

  <p class="section-content">
    Building on the Erlang runtime meant distributed systems were baked into every component and, while the team was very experienced with distributed systems [1] and the Patterns of Enterprise Application Architecture [2], our past toolkits rarely <i>enforced</i> message-passing distributed architectures; it was a choice we made.
    However, BEAM (and therefore, Elixir) declare, existentially, that message-passing and process boundaries (and their inverse: process size), are the primary considerations of system design.
    To build a functional system on BEAM at all, you must build a distributed system.
    Over the first few weeks of the project, the team deeply embedded themselves in the Erlang runtime paradigm --- learning, experimenting, and exploring along the way.
    The team had chosen Elixir precisely for its runtime and when it was time to load test the system, the distributed-system-up-front model had already proven itself.
  </p>

  <br/>
  <p class="section-content">
    Load testing is an exercise delineated by orders of magnitude.
    It becomes a non-trivial task when simulating concurrent users ordering in the hundreds of thousands.
    The team nicknamed these thousands of gossamer androids "The Mob"... built to swarm <i>ngage</i> with parallel but uncoordinated activity in hopes of breaking the system in the safety of a sandbox.
    The Mob[ile] Simulator (<i>mobsim</i> was a double entendre, since the anticipated million users were to connect exclusively through mobile devices) would continuously test <i>ngage</i>, looking for weaknesses in its distributed design, in turn helping the team carve out the all-important process boundaries.
    Neena would eventually transform the lessons from these iterations into his talk, "After The Crash", discussing systems design on the BEAM VM.
    [3] In his words, "many problems are simply avoided altogether because Erlang puts the complexity of scaling distributed systems front-and-centre, even when you run the system on a laptop."
  </p>
  <br/>
  <p class="section-content">
    Over the project's 18-month journey, starting with that first furious weekend and
    ending with the project handover, there were more landmark achievements than the
    million-concurrent-users requirement alone. A digital event isn't really an "event"
    if it's not interactive. Insider hadn't hired us to build YouTube. The first
    incarnation was a massive quiz show, which meant that the hundreds of thousands of
    participants needed to see the quiz questions at the <i>exact same time</i>. Initially,
    even the synchronization of questions to the video feed created difficulties. Video
    needed to be transcoded into a variety of formats for different users and, even after
    a number of transcoding optimizations, video delivered to mobile devices was
    consistently delayed by almost 10 full seconds. Synchronizing systems by timestamp
    was nothing new to the nilenso team but it was a new exercise to stamp metadata into
    streaming video. This self-identified the sync time and compensated for server-side
    transcoding delays in the mobile clients, ensuring questions always appeared to users
    at exactly the right time.
  </p>
  <br/>
  <p class="section-content">
    The Insider team running an event of course needed plenty of tools. The massive
    multi-user events could be meaningfully compared to Massively Multiplayer Online Games (MMOs) as
    each event was, in many ways, an MMO. But Insider events had the additional complexity
    of central coordination. If you are familiar with <i>Dungeons & Dragons</i>, you can think
    of the events team at Insider working together as a collective <i>Dungeon Master</i>. In a quiz
    show, for example, the presenter is the face of the show. But there are many other
    folks behind the scenes, managing teleprompters, publishing questions and answers to
    the massive live audience, and coordinating transitions from one question to the
    next. To coordinate this entire effort, nilenso built Insider a dashboard to control
    <i>ngage</i>. With this tool in hand, Insider wasn't just streaming mammoth amounts of
    videos and other data to millions of users simultaneously --- they were doing so in a carefully
    coordinated fashion.
  </p>
  <br/>
  <p class="section-content">
    Other project landmarks were not of a technical nature at all. The Insider team was
    incredibly busy with myriad projects at their headquarters in Bombay. Although both
    teams were operating on India Standard Time, it was often difficult for anyone in
    Bombay to make time for conversations... and a much bigger struggle to find time for
    prioritization. The counterintuitive solution was one many of nilenso's clients have
    chosen in the past: let nilenso drive the project. Insider knows events; nilenso
    knows software. The nilenso team would construct technical priorities and pursue
    them, reviewing the pending work with Insider whenever possible.
  </p>
  <br/>
  <p class="section-content">
    On other occasions, the technical and the non-technical would intersect. When it
    became clear that Insider did not have the time to build video transcoding and streaming into
    <i>ngage</i>, the team agreed that nilenso would build that as well. After
    evaluating OBS (Open Broadcaster Software) with a custom plugin, the team ultimately
    settled on a combination of mimoLive and Wowza Cloud. Although more expensive, this
    solution allowed Insider to focus on building its massive, interactive events
    platform rather than reinventing the media streaming wheel.
  </p>
  <br/>
  <p class="section-content">
    After months of research, exploration, and iterative development between the nilenso team, Insider's team in Bombay, and the mobile app team from Uncommon (now Able) in Hyderabad, it was time to take <i>ngage</i> out of its sandbox.
    Battle-hardened after the simulations provided by <i>mobsim</i>, it would finally see real phones across India connect to it.
    All three teams, spread across Bombay, Bangalore, and Hyderabad, would connect and listen to nilenso's tech lead shout "question!" while sending out the synchronized quiz question for everyone to respond.
    In each office, an echo of "que-ques-qu-question-tion-ion-n!" could be heard tumbling out of all the phones in the room.
    After everyone was satisfied with the behaviour of the system in the contrived scenario, Insider opened <i>ngage</i> up to the real world with its initial public betas.
  </p>
  <br/>
  <p class="section-content">
    Software projects of any complexity rarely make their maiden voyage without a hitch
    and the early betas were no exception. Beta 1 went smoothly from a technical
    standpoint, but the all-new format hadn't been marketed heavily enough and the first
    quiz show didn't draw a big crowd. Insider was determined to see a big turnout for
    Beta 2 --- advertisements went up, notices went out. And this time, a real crowd
    showed up. In anticipation, the team adjusted the settings of <i>ngage</i>, increasing the
    number of live processes, to correspond to the anticipated load. Unfortunately, this
    revealed a bug that <i>mobsim</i> hadn't uncovered yet. Satellite ("worker") Erlang
    processes are <i>designed</i> to safely handle crashes (hence the title of neena's talk:
    "After The Crash") but when too many crashes occur, the supervisor process in charge
    of restarting the workers can <i>crash itself</i>. If these failures cascade, the entire
    Erlang Supervision Tree [4] can collapse. During the massive Beta 2, this was
    precisely what happened.
  </p>
  <br/>
  <p class="section-content">
    Humbled, the team packed their bags and headed to Bombay.
    When the going gets tough, there's nothing quite like colocation in a single office.
    [5] With everyone under one roof and communication between all three teams flowing effortlessly, <i>ngage</i> learned to handle millions of concurrent users and <i>mobsim</i> became the platform on which it learned.
    Within weeks, the initial question-answer "quiz" format was exchanged for a more flexible vote-based "poll" format.
    Insider successfully ran new betas including a live comedy compeition, scored by the audience, and a live choose-your-own-adventure horror story, guided by audience choices.
    The Insider project, which was to become known as <i>Insider Gameshows</i>, was a success.
  </p>

  <h3 class="section-miniheading">The Outcomes</h3>
  <p class="section-content">
    No one knows whether a software project will be a success.
    Product competition, faulty business models, design and architecture mistakes... anything can lead to a project's downfall and, more often than not, this is the eventual conclusion.
    The success of <i>Insider Gameshows</i> meant the product had a market fit, the mobile app worked flawlessly for end users, and <i>ngage</i> stood up to the massive loads it needed to carry.
  </p>
  <br/>
  <p class="section-content">
    Ultimately, the new "poll" archetype gave Insider a very flexible format on which it could run endless online events.
    Elixir and the BEAM VM proved itself over and over.
    Some end users may have witnessed the flawed Beta 2 release, but the design characteristics innate to Erlang prevented countless other scalability problems.
    What about our friend <i>mobsim</i>? It's still in use today, acting as the load regression framework for every new release of <i>ngage</i>.
  </p>
  <br/>
  <p class="section-content">
    - TODO (?) transition to in-house team
  </p>
  <br/>
  <p class="section-content">
    The software industry frequently reaches for descriptions like "challenging work" and "world-class developers"... frequently enough that these words have become meaningless.
    There exists a dichotomy in the world of software consulting that we often fumble to elucidate.
    On one hand, some custom software lives in the world of the <i>bespoke</i>.
    A good tailor does not reinvent the craft of tailoring every time she cuts a new suit.
    Similarly, the three million apps on the stores of iOS and Android largely don't represent revolutions in Computer Science.
    Humanity isn't the proud custodian of billions of web apps because they each represent a brainchild of software engineering's avant-garde.
    Software in the 21st Century is wildly democratic, which is wonderful because each individual developer doesn't have the energy to rediscover the very definition of a mobile app every time she writes one.
    On the other hand, however, there does exist software which is truly experimental and genuinely difficult to produce.
    This is it.
  </p>

  <h3 class="section-miniheading">References</h3>
  <ol class="section-content">
    <li>[1] <a href="https://youtube.com/c/nilenso">Talks by nilenso employees</a></li>
    <li>[2] <a href="https://dl.acm.org/doi/book/10.5555/579257">Patterns of Enterprise Application Architecture</a></i>
    <li>[3] <a href="https://www.youtube.com/watch?v=9nbPZOHBsK4">After The Crash</a></i>
    <li>[4] <a href="http://erlang.org/documentation/doc-4.9.1/doc/design_principles/sup_princ.html">Erlang Supervision Principles</a></i>
    <li>[5] <a href="https://www.amazon.com/exec/obidos/ASIN/0321150783/poppendieckco-20">Lean Software Development</a></i>
  </ol>
</div>
